{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806cf218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\lg dx python\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns # 데이터셋을 가져오기 위해 import\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family = 'AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname = 'c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "    rc('font', family = font_name)\n",
    "    \n",
    "# 그래프에서 음수를 사용하기 위한 설정\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824ea5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
      "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
      "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
      "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
      "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
      "\n",
      "   model year  origin                       name  \n",
      "0          70       1  chevrolet chevelle malibu  \n",
      "1          70       1          buick skylark 320  \n",
      "2          70       1         plymouth satellite  \n",
      "3          70       1              amc rebel sst  \n",
      "4          70       1                ford torino  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392 entries, 0 to 397\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           392 non-null    float64\n",
      " 1   cylinders     392 non-null    int64  \n",
      " 2   displacement  392 non-null    float64\n",
      " 3   horsepower    392 non-null    float64\n",
      " 4   weight        392 non-null    float64\n",
      " 5   acceleration  392 non-null    float64\n",
      " 6   model year    392 non-null    int64  \n",
      " 7   origin        392 non-null    int64  \n",
      " 8   name          392 non-null    object \n",
      "dtypes: float64(5), int64(3), object(1)\n",
      "memory usage: 30.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# auto-mpg 데이터를 가져와서 이상치를 제거하고 숫자 타입으로 변환\n",
    "\n",
    "DF = pd.read_csv('./data/auto-mpg.csv', header = None)\n",
    "DF.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "DF['horsepower'].replace('?', np.nan, inplace = True)\n",
    "DF.dropna(subset = 'horsepower', inplace = True, axis = 0)\n",
    "DF['horsepower'] = DF['horsepower'].astype('float')\n",
    "\n",
    "print(DF.head())\n",
    "DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d146bdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     horsepower hp_bin\n",
      "393        86.0    저출력\n",
      "394        52.0    저출력\n",
      "395        84.0    저출력\n",
      "396        79.0    저출력\n",
      "397        82.0    저출력\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding 적용\n",
    "\n",
    "# horsepower 특성을 범주형으로 추가 - 3개 영역으로 구분\n",
    "# 3개의 구간으로 나누고 갯수와 경계값을 리턴 받아서 저장\n",
    "count, boundaries = np.histogram(DF['horsepower'], 3)\n",
    "\n",
    "# 범주형 형태로 생성\n",
    "# 사용할 데이터, 경계값, 영역 이름, lowest 포함 여부 순서\n",
    "DF['hp_bin'] = pd.cut(x = DF['horsepower'], bins = boundaries,\n",
    "                     labels = ['저출력', '보통', '고출력'], include_lowest = True)\n",
    "\n",
    "print(DF[['horsepower', 'hp_bin']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029757fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     저출력  보통  고출력\n",
      "0      0   1    0\n",
      "1      0   1    0\n",
      "2      0   1    0\n",
      "3      0   1    0\n",
      "4      0   1    0\n",
      "..   ...  ..  ...\n",
      "393    1   0    0\n",
      "394    1   0    0\n",
      "395    1   0    0\n",
      "396    1   0    0\n",
      "397    1   0    0\n",
      "\n",
      "[392 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 범주형에 대해 one hot 인코딩을 수행\n",
    "# 값이 3종류이므로 3개의 특성이 만들어지고 그 중 하나만 1이 됨\n",
    "horsepower_dummies = pd.get_dummies(DF['hp_bin'])\n",
    "print(horsepower_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a855f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "['고출력' '보통' '저출력']\n"
     ]
    }
   ],
   "source": [
    "# 1개의 특성에 대해 one hot 인코딩을 처리하는 클래스\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# 범주형 데이터에 LabelBinarizer 를 적용\n",
    "one_hot = LabelBinarizer()\n",
    "print(one_hot.fit_transform(DF['hp_bin']))\n",
    "\n",
    "# 이름 확인\n",
    "print(one_hot.classes_) # 순서는 달라질 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3786584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0]\n",
      " [0 1 1 0 0]\n",
      " [1 0 1 0 0]\n",
      " [0 0 1 0 1]\n",
      " [0 0 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [1 0 0 1 0]\n",
      " [0 0 0 1 1]]\n",
      "['a' 'b' 'c' 'd' 'e']\n"
     ]
    }
   ],
   "source": [
    "# 2개 이상의 특성을 가지고 one hot 인코딩\n",
    "# 2개 이상의 1이 등장할 수 있음\n",
    "\n",
    "# 2개 이상의 특성에 적용하는 MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 데이터 생성\n",
    "multi_features = [('a', 'b'), ('b', 'c'), ('c', 'a'), ('e', 'c'),\n",
    "                  ('c', 'd'), ('d', 'b'), ('a', 'd'), ('d', 'e')]\n",
    "\n",
    "# LabelBinarizer 와 동일한 방식으로 적용\n",
    "multi_one_hot = MultiLabelBinarizer()\n",
    "print(multi_one_hot.fit_transform(multi_features))\n",
    "print(multi_one_hot.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad5ccba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2 2 2 2 2 2 2 2 2 1 2 0 0 0 0 2 2 2 2 2 2 2 2\n",
      " 1 0 1 1 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 1 1 1 0 1 1 0 2 1 1 1\n",
      " 1 1 2 2 2 2 2 2 2 2 0 1 1 1 1 0 1 1 1 0 0 0 2 2 2 2 2 2 1 1 0 0 2 2 2 2 2\n",
      " 2 2 2 1 0 2 2 2 1 1 1 1 0 2 2 2 2 2 2 2 2 1 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 0 1 1 1 1 2 1 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 1 0 1 1 1 2 2 2 2 2 1 1 1\n",
      " 1 1 2 2 2 0 0 0 1 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 1 2 2 2 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2]\n",
      "['고출력' '보통' '저출력']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 범주형 데이터에 LabelEncoder 를 적용\n",
    "# 적용 방식은 동일함\n",
    "label = LabelEncoder()\n",
    "print(label.fit_transform(DF['hp_bin']))\n",
    "\n",
    "# 이름 확인\n",
    "print(label.classes_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a8af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  score  encoder\n",
      "0     수        5\n",
      "1     우        4\n",
      "2     미        3\n",
      "3     수        5\n",
      "4     양        2\n",
      "5     가        1\n",
      "6     미        3\n",
      "7     우        4\n"
     ]
    }
   ],
   "source": [
    "# 순서가 의미를 갖는 경우 - replace 활용하기\n",
    "\n",
    "# 데이터 생성\n",
    "df = pd.DataFrame({'score' : ['수', '우', '미', '수', '양', '가', '미', '우']})\n",
    "# 순서를 지정하기 위해 dict 생성\n",
    "scale_mapper = {'가' : 1, '양' : 2, '미' : 3, '우' : 4, '수' : 5}\n",
    "\n",
    "df['encoder'] = df['score'].replace(scale_mapper)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78c19d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [2. 2.]\n",
      " [0. 1.]]\n",
      "[array(['high', 'low', 'middle'], dtype='<U11'), array(['10', '14', '20'], dtype='<U11')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 데이터 생성\n",
    "features = np.array([['low' , 10], ['middle' , 20], ['high' , 14]])\n",
    "\n",
    "# 객체 생성\n",
    "ordinal = OrdinalEncoder()\n",
    "# 생성한 객체에 데이터 삽입\n",
    "print(ordinal.fit_transform(features))\n",
    "\n",
    "print(ordinal.categories_) # 사전 순서대로 순서를 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eb54ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2.]\n",
      "[[0.  0.2 4. ]\n",
      " [2.  3.1 1.6]]\n",
      "[[0.  0.2 4. ]\n",
      " [2.  3.1 1.6]\n",
      " [0.  2.  1.3]\n",
      " [4.  2.  0.2]\n",
      " [0.  2.  4.1]\n",
      " [2.  3.  2.5]]\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델을 이용한 결측값 대체\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 훈련 데이터 생성\n",
    "data = np.array([[0, 2, 1.3], [4, 2, 0.2], [0, 2 , 4.1], [2, 3, 2.5]])\n",
    "\n",
    "# 예측에 사용할 데이터\n",
    "data_with_na = np.array([[np.nan, 0.2, 4], [np.nan, 3.1, 1.6]])\n",
    "\n",
    "# KNN 학습기 생성 - 거리를 사용\n",
    "clf = KNeighborsClassifier(3, weights = 'distance')\n",
    "\n",
    "# 첫번째 데이터를 label로 하고 나머지 데이터를 feature로 설정해서 훈련 진행\n",
    "trained_model = clf.fit(data[:, 1:], data[:, 0])\n",
    "\n",
    "# 생성한 모델과 na가 포함된 데이터의 다른 데이터들을 통해 예측 - predict\n",
    "imputed_values = trained_model.predict(data_with_na[:, 1:])\n",
    "\n",
    "# 결과 출력\n",
    "print(imputed_values)\n",
    "\n",
    "# 예측한 데이터와 원본 데이터를 합치기 - 데이터의 na 를 예측값으로 대체\n",
    "# hstack 은 옆으로 데이터를 합침 - 괄호 2개 필요\n",
    "data_with_imputed = np.hstack((imputed_values.reshape(-1, 1), data_with_na[:, 1:]))\n",
    "print(data_with_imputed)\n",
    "\n",
    "# 결측치를 대체한 데이터와 훈련에 사용한 데이터를 합침\n",
    "# vstack은 데이터를 위아래로 합침 - 괄호 2개 필요\n",
    "result = np.vstack((data_with_imputed, data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0605d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan 0.2 4. ]\n",
      " [nan 3.1 1.6]\n",
      " [0.  2.  1.3]\n",
      " [4.  2.  0.2]\n",
      " [0.  2.  4.1]\n",
      " [2.  3.  2.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.2, 4. ],\n",
       "       [0. , 3.1, 1.6],\n",
       "       [0. , 2. , 1.3],\n",
       "       [4. , 2. , 0.2],\n",
       "       [0. , 2. , 4.1],\n",
       "       [2. , 3. , 2.5]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 많이 나오는 데이터로 대체\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# nan 이 포함된 데이터를 기존 데이터와 합침\n",
    "data_complete = np.vstack((data_with_na, data))\n",
    "print(data_complete)\n",
    "\n",
    "# 최빈값으로 결측치를 대체\n",
    "imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "# 데이터에 적용\n",
    "imputer.fit_transform(data_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3689eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f5bcec",
   "metadata": {},
   "source": [
    "## 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4e9f1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='1'>\n",
      "None\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match=' 1'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 매칭 여부를 확인\n",
    "# 패턴에 일치하는 데이터가 있으면 Match 객체를 리턴하고 없으면 None 리턴\n",
    "# 앞의 표현식과 뒤의 데이터를 비교\n",
    "match = re.match('[0-9]', '1234')\n",
    "print(match) # Match object 출력\n",
    "match = re.match('[0-9]', 'jongho')\n",
    "print(match) # None 출력\n",
    "match = re.match('[0-9]', 'jongho33')\n",
    "print(match) # None\n",
    "match = re.match('[0-9]', ' 1234 ')\n",
    "print(match) # 공백이 포함되어 있으므로 일치하지 않아서 None\n",
    "match = re.match('^\\s[0-9]', ' 1234')\n",
    "print(match) # 패턴에 공백이 포함되어서 Match object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "517d131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!@#as fQ R  #.가나다라 \n"
     ]
    }
   ],
   "source": [
    "string = '!@#as fQ R81  #.가나다라 '\n",
    "\n",
    "# 숫자 데이터 제거\n",
    "\n",
    "# 숫자 패턴을 입력(compile)\n",
    "p_num = re.compile('[0-9]+')\n",
    "# 숫자 패턴과 일치하는 것을 ''(제거)로 대체\n",
    "result = p_num.sub('', string)\n",
    "print(result) # '!@#as fQ R  #.가나다라 ' - 숫자 사라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d7e4f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asfQR가나다라\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 제거\n",
    "\n",
    "# \\W 는 숫자나 문자를 제외한 전부\n",
    "p_spe = re.compile('\\W+')\n",
    "result = p_spe.sub('', result)\n",
    "# 공백도 사라짐\n",
    "print(result) # 'asfQR가나다라'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136f18c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이렇게', '저렇게', '이런', '저런']\n"
     ]
    }
   ],
   "source": [
    "# unicodedata 사용\n",
    "\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "string_data = ['이렇게.', '저렇게!', '이런,', '저런\"']\n",
    "\n",
    "# 구두점에 대한 dict 생성\n",
    "punc = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                     if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# 구두점을 포함해서 특수 문자들을 삭제\n",
    "result = [string.translate(punc) for string in string_data]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cbb618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# nltk 설치\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31ff06b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk 패키지 설치\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9c42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'erdfas', '빌뮇바누히']\n",
      "['.', '1번 문장입니다.', '2번 문장.', '3번 문장, ㅁㄴㅇㄹ.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # 단어 단위로 토큰화\n",
    "from nltk.tokenize import sent_tokenize # 문장 단위로 토큰화\n",
    "\n",
    "sentence = 'a b c d erdfas 빌뮇바누히'\n",
    "\n",
    "# 공백 단위로 구분해서 토큰화\n",
    "tok = word_tokenize(sentence)\n",
    "print(tok)\n",
    "# ['a', 'b', 'c', 'd', 'erdfas', '빌뮇바누히']\n",
    "\n",
    "# 구두점과 띄어쓰기를 기준으로 문장을 구분해서 토큰화\n",
    "sentences = '. 1번 문장입니다. 2번 문장. 3번 문장, ㅁㄴㅇㄹ.'\n",
    "print(sent_tokenize(sentences)) \n",
    "# ['.', '1번 문장입니다.', '2번 문장.', '3번 문장, ㅁㄴㅇㄹ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238355b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d101fea7",
   "metadata": {},
   "source": [
    "## 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35a903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3일', '2023년', '10시', '08분']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 - 한글은 불용어 사전이 따로 없기 때문에 직접 만들어야 함\n",
    "word_korean = ['1월', '3일', '2023년', '10시', '08분', '25초']\n",
    "korean_stopwords = ['1월', '25초']\n",
    "\n",
    "# 'i for i ~ ' 는 작업을 수행해서 generator 를 생성\n",
    "# generator 는 이터레이터로 접근할 수 있는 객체임\n",
    "print([i for i in word_korean if i not in korean_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b531577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the good', 'nice', 'c', 'take', 'movie', 'hands']\n"
     ]
    }
   ],
   "source": [
    "# 영문은 nltk 에서 기본적인 불용어 사전을 제공함\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "word_eng = ['and', 'the good', 'nice', 'c', 'take', 'movie', 'hands', 'a']\n",
    "\n",
    "# 영문 불용어 사전에 포함되지 않는 단어만 가져오도록\n",
    "result = [word for word in word_eng if word not in stopwords.words('english')]\n",
    "\n",
    "print(result) # and, a 를 제외함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cb94029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the good', 'nice', 'c', 'movie', 'hands']\n"
     ]
    }
   ],
   "source": [
    "# sklearn 의 불용어의 수가 nltk 보다 더 많음\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "result = [word for word in word_eng if word not in ENGLISH_STOP_WORDS]\n",
    "print(result) # 위에 더해서 'take' 도 제외됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf321a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Pythons', 'have', 'python', \"'s\", 'python', '.']\n",
      "all\tpython\thave\tpython\t's\tpython\t.\t\n",
      "al\tpython\thav\tpython\t's\tpython\t.\t"
     ]
    }
   ],
   "source": [
    "# 영문의 어간 추출\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "string = \"All Pythons have python's python.\"\n",
    "\n",
    "# 단어 토큰화 - word tokenize, 공백을 기준으로 분할하여 list 생성\n",
    "result1 = word_tokenize(string)\n",
    "print(result1) # python 과 's, . 을 분리하여 토큰화\n",
    "\n",
    "# 어간 추출 - PerterStemmer\n",
    "ps_stemmer = PorterStemmer()\n",
    "for word in result1:\n",
    "    print(ps_stemmer.stem(word), end = '\\t')\n",
    "# 대문자와 소문자를 구별하지 않으며 복수형과 단수형도 상관없이 추출\n",
    "\n",
    "print()\n",
    "\n",
    "# 어간 추출 - LancasterStemmer\n",
    "ps_stemmer = LancasterStemmer()\n",
    "for word in result1:\n",
    "    print(ps_stemmer.stem(word), end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfb0590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석 - 품사 태깅\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13aae30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('that', 'DT'), ('movie', 'NN'), ('star', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "tokens = word_tokenize('The movie was made by that movie star!')\n",
    "# 토큰을 가지고 단어와 품사의 튜플 리스트로 나눔\n",
    "en_tags = pos_tag(tokens)\n",
    "# 품사 : NNP - 고유명사, NN : 명사, RB : 부사, VBD : 동사 ...\n",
    "\n",
    "print(en_tags) # [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ... ('!', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae48ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'movie', 'star']\n"
     ]
    }
   ],
   "source": [
    "# 품사가 명사(NN)인 단어만 출력\n",
    "# tuple 이기 때문에 나눠서 받아올 수 있음\n",
    "print([word for word, tag in en_tags if tag == 'NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0de1e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from konlpy) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (22.0)\n",
      "Installing collected packages: konlpy\n",
      "Successfully installed konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f862ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.7.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.7.2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge jpype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650d5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80fa7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘도 날씨가 매우 덥다.', '언제쯤 시원 해지려나.', '다음주에 비가 오면 좋겠다.', '그 다음 문장입니다.', '그 다다음 문장.']\n",
      "['오늘', '날씨', '언제쯤', '다음주', '비가', '다음', '문장', '다다']\n",
      "[('오늘', 'NNG'), ('도', 'JX'), ('날씨', 'NNG'), ('가', 'JKS'), ('매우', 'MAG'), ('덥', 'VA'), ('다', 'EFN'), ('.', 'SF'), ('언제쯤', 'NNG'), ('시원', 'XR'), ('해지', 'VV'), ('려나', 'EFQ'), ('.', 'SF'), ('다음주', 'NNG'), ('에', 'JKM'), ('비가', 'NNG'), ('오', 'VV'), ('면', 'ECE'), ('좋', 'VA'), ('겠', 'EPT'), ('다', 'EFN'), ('.', 'SF'), ('그', 'MDT'), ('다음', 'NNG'), ('문장', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('그', 'MDT'), ('다다', 'NNP'), ('음', 'XSN'), ('문장', 'NNG'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "text = '''오늘도 날씨가 매우 덥다. 언제쯤 시원해지려나. 다음주에 비가 오면 좋겠다.\n",
    "그 다음 문장입니다. 그 다다음 문장.'''\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "# 문장 분석\n",
    "print(kkma.sentences(text))\n",
    "\n",
    "# 단어 분석\n",
    "print(kkma.nouns(text))\n",
    "\n",
    "# 형태소 분석\n",
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "598db0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '언제쯤', '시원', '다음주', '비', '다음', '문장', '다다음', '문장']\n",
      "['오늘', '도', '날씨', '가', '매우', '덥', '다', '.', '언제쯤', '시원', '하', '어', '지', '려', '나', '.', '다음주', '에', '비', '가', '오', '면', '좋', '겠다', '.', '그', '다음', '문장', '이', 'ㅂ니다', '.', '그', '다다음', '문장', '.']\n",
      "[('오늘', 'N'), ('도', 'J'), ('날씨', 'N'), ('가', 'J'), ('매우', 'M'), ('덥', 'P'), ('다', 'E'), ('.', 'S'), ('언제쯤', 'N'), ('시원', 'N'), ('하', 'X'), ('어', 'E'), ('지', 'P'), ('려', 'E'), ('나', 'J'), ('.', 'S'), ('다음주', 'N'), ('에', 'J'), ('비', 'N'), ('가', 'J'), ('오', 'P'), ('면', 'E'), ('좋', 'P'), ('겠다', 'E'), ('.', 'S'), ('그', 'M'), ('다음', 'N'), ('문장', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S'), ('그', 'M'), ('다다음', 'N'), ('문장', 'N'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "# 다른 형태소 분석기인 Hannanum\n",
    "# 성능은 Kkma가 우수하다고 알려져 있지만\n",
    "# 메모리 사용량이 많고 속도가 조금 느리다는게 단점\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "text = '''오늘도 날씨가 매우 덥다. 언제쯤 시원해지려나. 다음주에 비가 오면 좋겠다.\n",
    "그 다음 문장입니다. 그 다다음 문장.'''\n",
    "han = Hannanum()\n",
    "\n",
    "\n",
    "# 단어 분석\n",
    "print(han.nouns(text))\n",
    "# 형태소 분석\n",
    "print(han.morphs(text))\n",
    "print(han.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af1ee1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2 0 0 1]\n",
      " [0 1 0 0 0 0 0 1 1 1 1 1 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 1 0 0]]\n",
      "['after' 'day' 'dear' 'feature' 'friend' 'got' 'import' 'is' 'light'\n",
      " 'like' 'my' 'phone' 'to']\n"
     ]
    }
   ],
   "source": [
    "# BoW(Bag of Word - 단어의 갯수)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I got a day after import feature', 'I like to like I',\n",
    "                     'My phone is light like a day', 'Dear my friend'])\n",
    "\n",
    "# 객체 생성\n",
    "cnt = CountVectorizer()\n",
    "# 데이터 삽입\n",
    "bag_of_words = cnt.fit_transform(text_data)\n",
    "\n",
    "# 희소 행렬의 형태로 결과가 출력됨\n",
    "#print(bag_of_words) \n",
    "# 밀집 행렬의 형태로 전환해서 출력\n",
    "print(bag_of_words.toarray())\n",
    "# 특성의 이름 확인 - 자동으로 정렬을 진행하며 stopwords 도 처리\n",
    "print(cnt.get_feature_names_out()) # I, a 등의 stopwords 가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22a1fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'got': 2, 'day': 1, 'after': 0, 'like': 5, 'to': 10, 'my': 7, 'phone': 8, 'is': 3, 'light': 4, 'likes': 6, 'phones': 9}\n",
      "[[0.58873218 0.34771471 0.72971837 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.24345993 0.         0.         0.         0.82442698\n",
      "  0.         0.         0.         0.         0.510928  ]\n",
      " [0.         0.23892851 0.         0.50141831 0.50141831 0.40454114\n",
      "  0.         0.33580569 0.40454114 0.         0.        ]\n",
      " [0.         0.29131278 0.         0.         0.         0.\n",
      "  0.61135259 0.40942995 0.         0.61135259 0.        ]\n",
      " [0.57373967 0.33885989 0.         0.         0.         0.\n",
      "  0.         0.47625576 0.57373967 0.         0.        ]]\n",
      "['after' 'day' 'got' 'is' 'light' 'like' 'likes' 'my' 'phone' 'phones'\n",
      " 'to']\n"
     ]
    }
   ],
   "source": [
    "# tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['I got a day after', 'I like to like a day',\n",
    "                     'My phone is light like a day', 'A day likes my phones',\n",
    "                     'Day after my phone'])\n",
    "\n",
    "# tf-idf 객체를 생성\n",
    "tf_idf = TfidfVectorizer()\n",
    "# 데이터 삽입\n",
    "result = tf_idf.fit_transform(text_data)\n",
    "\n",
    "# 결과 출력\n",
    "#print(result)\n",
    "print(tf_idf.vocabulary_)\n",
    "print(result.toarray())\n",
    "print(tf_idf.get_feature_names_out()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
